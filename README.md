# Prompt using Masked Language Modeling for finetuning LLMs
Reproducing ideas from __Making Pretrained Language Models Better Few Shot Lerners__

Main results -

<img src="main_results.png"  width="600px" height="300px">

Experiment results on `roberta-base`

<img src="roberta-base-exp.jpg"  width="600px" height="400px">


                                                            
