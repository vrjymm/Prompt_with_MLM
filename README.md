# Prompt with Masked Language Modeling for finetuning LLMs
Masked Languag Modelling entails masking a part of input sequence and then training a model to predict the masked tokens. How can we prompt a pretrained model to predict the sentiment, or topic classification. 
Reproducing ideas from __Making Pretrained Language Models Better Few Shot Lerners__

### Examples
<hr>
Main results -

<img src="main_results.png"  width="600px" height="300px">

Experiment results on `roberta-base`

<img src="roberta-base-exp.jpg"  width="600px" height="400px">


                                                            
