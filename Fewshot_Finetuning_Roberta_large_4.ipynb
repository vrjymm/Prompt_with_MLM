{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkIHBFT3ouqZ",
    "outputId": "3355ff74-14bf-4487-8706-f0db386d5258"
   },
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install transformers==3.0.2\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import pipeline\n",
    "\n",
    "from torch import cuda\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3IV78Yno5qg",
    "outputId": "b88b04ea-80e3-41de-c695-06e361b544b8"
   },
   "outputs": [],
   "source": [
    "# Mounting Google Drive to this .ipynb\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "train_data_loc = 'SST-2/Few_Shot/train_4.tsv'\n",
    "dev_data_loc = 'SST-2/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Fmq-fLko-FN"
   },
   "outputs": [],
   "source": [
    "# Some Parameters\n",
    "\n",
    "max_len = 256\n",
    "train_batch_size = 8\n",
    "val_batch_size = 8\n",
    "roberta_flavour = 'roberta-large'\n",
    "learning_rate = 2e-5\n",
    "tokenizer = RobertaTokenizer.from_pretrained(roberta_flavour, truncation = True, do_lower_case = True)\n",
    "\n",
    "# Dataloader - Custom Dataset Class\n",
    "\n",
    "class SST2_Basic(Dataset):\n",
    "    def __init__(self, file_loc, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_len\n",
    "        \n",
    "        with open(file_loc) as f:\n",
    "            f.readline()\n",
    "            data = [line.split(\"\\t\") for line in f]\n",
    "            \n",
    "        self.docs = [x for (x,y) in data]\n",
    "        self.targets = [int(y) for (x,y) in data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        doc = str(self.docs[index])\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            doc,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_length,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylOnM6q8pGnm"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_set = SST2_Basic(train_data_loc, tokenizer, max_len)\n",
    "validation_set = SST2_Basic(dev_data_loc, tokenizer, max_len)\n",
    "\n",
    "train_params = {'batch_size': train_batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "val_params = {'batch_size': val_batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_data = DataLoader(training_set, **train_params)\n",
    "val_data = DataLoader(validation_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yRuF1uJpRPo"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# Step 1: Define the model\n",
    "\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        \n",
    "        self.l1 = RobertaModel.from_pretrained(roberta_flavour)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        main = self.l1(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        hidden_state = main[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        output = self.classifier(self.dropout(pooler))\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnRdubMdpUMG",
    "outputId": "25707fda-94b6-4a91-998c-9b342e612fc4"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfAjTxoTpVdg"
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "\n",
    "def calculate_accuracy(preds, targets):\n",
    "    n_correct = (preds == targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k57xMoAkpfSo"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, train_data_loader, validation_data_loader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(train_data_loader, 0)):\n",
    "        \n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%500==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pL89l6pphh2"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDA9YV4Jpijm",
    "outputId": "e4a0a9ba-33e1-4cce-9391-b50a098c08b2"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train(model, epoch, train_data, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChJjUijTtV2_",
    "outputId": "7d8f53a5-8813-4e75-c657-3e3c200ca21d"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch per 1000 steps: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch per 1000 steps: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-APa78HtYDD",
    "outputId": "e8e72af0-39c0-4b0c-d537-dcaa87d4b95f"
   },
   "outputs": [],
   "source": [
    "\n",
    "vacc = valid(model, val_data)\n",
    "print(\"Val Acc: \", vacc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fewshot_Finetuning_Roberta_base_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
